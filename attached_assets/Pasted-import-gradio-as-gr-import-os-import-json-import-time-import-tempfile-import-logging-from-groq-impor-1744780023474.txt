import gradio as gr
import os
import json
import time
import tempfile
import logging
from groq import Groq
from dotenv import load_dotenv

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('.claude/debug.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Configure API keys
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
client = Groq(api_key=GROQ_API_KEY)

# Constants
LLAMA_MODEL = "meta-llama/llama-4-maverick-17b-128e-instruct"
COMPOUND_MODEL = "compound-beta"

def generate_follow_up_questions(query):
    """Generate 5 follow-up questions using Llama-4-Maverick model"""
    prompt = f"""Based on the following research query, generate 5 specific follow-up questions 
    that would help gather more comprehensive information for compound research.
    The questions should explore different aspects of the topic and help elicit detailed information.
    
    Research Query: {query}
    
    Format your response as a JSON array of 5 questions only. No preamble or explanation.
    Example: ["Question 1?", "Question 2?", "Question 3?", "Question 4?", "Question 5?"]
    """

    completion = client.chat.completions.create(
        model=LLAMA_MODEL,
        messages=[{"role": "user", "content": prompt}]
    )
    
    response = completion.choices[0].message.content
    
    # Try to parse the response as JSON
    try:
        questions = json.loads(response)
        if not isinstance(questions, list):
            # If not a list, try to extract a JSON array from the text
            import re
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if json_match:
                questions = json.loads(json_match.group(0))
            else:
                questions = []
    except json.JSONDecodeError:
        # If parsing fails, try to extract an array from the text
        import re
        json_match = re.search(r'\[.*\]', response, re.DOTALL)
        if json_match:
            try:
                questions = json.loads(json_match.group(0))
            except:
                questions = []
        else:
            questions = []
    
    # Ensure we have 5 questions
    if len(questions) > 5:
        questions = questions[:5]
    
    return questions

def generate_report_title(query, research_data):
    """Generate a compelling title for the research report using Llama-4-Maverick"""
    prompt = f"""You are tasked with creating a compelling, descriptive title for a comprehensive research report.
    
    Research Query: {query}
    
    Research Information:
    {research_data[:2000]}  # Using just the beginning of research data for context
    
    Create a professional, academic-style title that:
    1. Is concise but descriptive (7-12 words)
    2. Accurately captures the essence of the research topic
    3. Is engaging and would appeal to readers interested in this subject
    4. Follows academic title conventions
    5. Avoids clickbait or sensationalism
    
    Return ONLY the title text, with no quotes, prefixes, or explanations.
    """
    
    completion = client.chat.completions.create(
        model=LLAMA_MODEL,
        messages=[{"role": "user", "content": prompt}]
    )
    
    # Clean up the response - remove any quotes or extra formatting
    title = completion.choices[0].message.content.strip().strip('"\'')
    return title

def answer_follow_up_questions(query, questions):
    """Use Compound-Beta to answer follow-up questions with sources and hyperlinks"""
    answers = []
    all_qa_pairs = []
    
    for i, question in enumerate(questions, 1):
        prompt = f"""You are a knowledgeable research assistant. Please answer the following question
        based on the context of this research query: "{query}"

        Question: {question}
        
        Provide a factual answer with relevant information. Include sources or data if available.
        VERY IMPORTANT: When citing sources, include hyperlinks to those sources in your answer.
        Use the format [Source Name](URL) for all citations.
        Focus on accuracy and relevance to the research topic.
        """
        
        completion = client.chat.completions.create(
            model=COMPOUND_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        
        answer = completion.choices[0].message.content
        answers.append(answer)
        all_qa_pairs.append({"question": question, "answer": answer})
        
    return answers, all_qa_pairs

def gather_research_data(query, qa_pairs):
    """Use Compound-Beta to gather targeted research data with hyperlinked sources"""
    context = f"Main Query: {query}\n\nAdditional Information:\n"
    
    for i, qa in enumerate(qa_pairs, 1):
        context += f"{i}. Question: {qa['question']}\nAnswer: {qa['answer']}\n\n"
    
    prompt = f"""You are a research assistant tasked with gathering detailed research data.
    I need you to search for information related to this research query and the follow-up questions.
    
    {context}
    
    Gather comprehensive research data with these requirements:
    1. Search for relevant facts, statistics, and information
    2. Find authoritative sources for each piece of information
    3. VERY IMPORTANT: Include HYPERLINKED citations for ALL information using markdown format: [Source Name](URL)
    4. Gather diverse perspectives on the topic
    5. Focus on recent and reliable information
    6. Structure information clearly with headings when appropriate
    
    For EACH piece of information, follow this pattern:
    - State the fact or information clearly
    - Provide the source with a hyperlink: [Source Name](URL)
    - Add brief context about why this information is relevant
    
    Format your response in clear sections based on different aspects of the topic.
    """
    
    completion = client.chat.completions.create(
        model=COMPOUND_MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=8192
    )
    
    return completion.choices[0].message.content

def create_research_outline(query, questions, answers, research_data):
    """Generate a comprehensive outline using Llama-4-Maverick"""
    context = f"""Research Query: {query}\n\nFollow-up Questions and Answers:\n\n"""
    
    for q, a in zip(questions, answers):
        context += f"Q: {q}\nA: {a}\n\n"
    
    context += f"\nResearch Data:\n{research_data}\n\n"
    
    prompt = f"""You are tasked with creating a well-structured outline for a comprehensive research report.
    
    Based on the following information:
    
    {context}
    
    Create a detailed outline with 5-7 main sections that cover different aspects of the topic.
    
    Format your outline using markdown with clear hierarchical structure:
    - Use ## for main sections
    - Use ### for subsections if needed
    
    Each section should have a descriptive title that accurately reflects its content.
    Include a brief (1-2 sentences) description of what each section will cover.
    
    The outline should be logical, well-organized, and comprehensive.
    
    IMPORTANT: DO NOT include "Executive Summary" or "Conclusion" sections - these will be added separately.
    Focus only on the main content sections of the report.
    """
    
    completion = client.chat.completions.create(
        model=LLAMA_MODEL,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return completion.choices[0].message.content

def parse_outline(outline_text):
    """Parse the outline into a structured format for section-by-section generation"""
    # Simple parsing based on markdown headers
    sections = []
    current_section = None
    
    lines = outline_text.split('\n')
    for line in lines:
        line = line.strip()
        if line.startswith('## '):  # Main section
            if current_section:
                sections.append(current_section)
            current_section = {
                'title': line[3:],
                'description': '',
                'subsections': []
            }
        elif line.startswith('### '):  # Subsection
            if current_section:
                current_section['subsections'].append({
                    'title': line[4:],
                    'description': ''
                })
        elif current_section and line and not line.startswith('#'):
            if current_section['description']:
                current_section['description'] += ' ' + line
            else:
                current_section['description'] = line
    
    if current_section:
        sections.append(current_section)
    
    # If parsing failed or produced no sections, create a default structure
    if not sections:
        sections = [
            {'title': 'Main Findings', 'description': 'Key research findings', 'subsections': []},
            {'title': 'Analysis', 'description': 'Analysis of the research', 'subsections': []},
            {'title': 'Discussion', 'description': 'Discussion of implications', 'subsections': []}
        ]
    
    # Filter out any executive summary or conclusion sections that might have been included
    filtered_sections = []
    for section in sections:
        title_lower = section['title'].lower()
        if "executive summary" not in title_lower and "conclusion" not in title_lower:
            filtered_sections.append(section)
    
    return filtered_sections

def generate_section_content(query, section, previous_content, research_data, qa_context):
    """Generate content for a specific section using Llama-4-Maverick"""
    # Create context with all the necessary information
    context = f"""Research Query: {query}

Previous Content Generated:
{previous_content}

Section to Write: {section['title']}
Section Description: {section['description']}

Follow-up Questions and Answers:
{qa_context}

Research Data:
{research_data}
"""

    prompt = f"""You are writing a section of a comprehensive research report.
    
    {context}
    
    Write 3-4 well-crafted paragraphs for this section that:
    1. Are directly relevant to the section title and description
    2. Include factual information with HYPERLINKED CITATIONS using markdown format: [Source Name](URL)
    3. Provide meaningful analysis and insights
    4. Avoid repeating information that appears in previous content
    5. Are well-structured and flow logically
    6. Use academic, professional language
    
    IMPORTANT REQUIREMENTS:
    - Include at least 3 specific citations to sources, using hyperlinks in markdown format: [Source Name](URL)
    - Draw from the research data provided, but formulate your own insights
    - Each paragraph should be substantial (5-7 sentences) and develop a clear point
    - DO NOT repeat information from previous sections; provide new perspectives and details
    - DO NOT include the section heading in your content; it will be added separately
    - Use subheadings if appropriate to organize the content
    - End with a smooth transition to the next section
    
    Format your response using proper markdown, with clear paragraph breaks.
    """
    
    completion = client.chat.completions.create(
        model=LLAMA_MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=8192
    )
    
    return completion.choices[0].message.content

def generate_executive_summary(query, full_report):
    """Generate an executive summary using Llama-4-Maverick"""
    prompt = f"""You are tasked with writing an executive summary for a comprehensive research report.
    
    Research Query: {query}
    
    Based on the following full report:
    
    {full_report}
    
    Write a concise executive summary that:
    1. Is approximately 2-3 paragraphs in length
    2. Captures the key findings and insights from the full report
    3. Mentions the major conclusions without detailed citations
    4. Provides a high-level overview that would help a busy reader understand the core message
    5. Uses clear, professional language
    
    Format your response using markdown. DO NOT include a heading like "Executive Summary" in your response;
    the heading will be added separately.
    """
    
    completion = client.chat.completions.create(
        model=LLAMA_MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=8192
    )
    
    return completion.choices[0].message.content

def generate_conclusion(query, full_report):
    """Generate a conclusion using Llama-4-Maverick"""
    prompt = f"""You are tasked with writing the conclusion for a comprehensive research report.
    
    Research Query: {query}
    
    Based on the following full report:
    
    {full_report}
    
    Write a thoughtful conclusion that:
    1. Is approximately 2-3 paragraphs in length
    2. Summarizes the key findings and insights from the report
    3. Discusses broader implications of the research
    4. Identifies any limitations of the current research
    5. Suggests potential areas for future research
    6. Ends with a compelling final thought that gives closure to the report
    
    Format your response using markdown. DO NOT include a heading like "Conclusion" in your response;
    the heading will be added separately.
    """
    
    completion = client.chat.completions.create(
        model=LLAMA_MODEL,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return completion.choices[0].message.content

def save_report_to_file(report_text, title="Research_Report"):
    """Save the report to a file and return the file path"""
    try:
        # Clean the title for a filename
        clean_title = title.replace(' ', '_').replace(':', '').replace('?', '').replace('!', '').replace(',', '').replace(';', '')
        filename = f"{clean_title}.md"
        
        # Create a temporary file
        temp_dir = tempfile.gettempdir()
        file_path = os.path.join(temp_dir, filename)
        
        logger.info(f"Saving report to: {file_path}")
        
        # Write content to file
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
            
        return file_path
    except Exception as e:
        logger.error(f"Error saving report: {str(e)}")
        return None

def compound_research_process(query):
    """Stream results from the compound research process with section-by-section generation"""
    try:
        # Initialize with starting message
        output = f"# Compound Research: '{query}'\n\n## Progress:\n\n"
        yield output, None, None
        
        # Step 1: Generate follow-up questions
        output += "- ‚è≥ Generating follow-up questions...\n"
        yield output, None, None
        
        questions = generate_follow_up_questions(query)
        
        # Update progress
        output += "- ‚úÖ Follow-up questions generated\n- ‚è≥ Answering questions using AI...\n"
        
        # Format questions for display
        output += "\n\n## Generated Questions:\n\n"
        for i, question in enumerate(questions, 1):
            output += f"{i}. {question}\n"
        output += "\n"
        yield output, None, None
        
        # Step 2: Answer follow-up questions with Compound Beta
        output += "\n## Questions and Answers:\n\n"
        yield output, None, None
        
        # Create QA context for later use
        qa_context = ""
        
        # Answer each question and update the output
        for i, question in enumerate(questions):
            # Update progress indicator
            current_progress = f"- ‚è≥ Answering question {i+1} of {len(questions)}...\n"
            output_with_progress = output + current_progress
            yield output_with_progress, None, None
            
            # Get the answer
            prompt = f"""You are a knowledgeable research assistant. Please answer the following question
            based on the context of this research query: "{query}"

            Question: {question}
            
            Provide a concise, factual answer with relevant information. Include sources or data if available.
            VERY IMPORTANT: When citing sources, include hyperlinks to those sources in your answer.
            Use the format [Source Name](URL) for all citations.
            Focus on accuracy and relevance to the research topic.
            """
            
            completion = client.chat.completions.create(
                model=COMPOUND_MODEL,
                messages=[{"role": "user", "content": prompt}]
            )
            
            answer = completion.choices[0].message.content
            
            # Add to QA context
            qa_context += f"Q: {question}\nA: {answer}\n\n"
            
            # Add the QA to the output
            output += f"### Q{i+1}: {question}\n\n{answer}\n\n---\n\n"
            yield output, None, None
        
        # Step 3: Gather research data
        output += "- ‚úÖ Questions answered\n- ‚è≥ Gathering research data with sources...\n"
        yield output, None, None
        
        # Get all answers in bulk for the next steps
        answers, qa_pairs = answer_follow_up_questions(query, questions)
        
        # Gather research data with hyperlinked sources
        research_data = gather_research_data(query, qa_pairs)
        
        # Generate a report title
        output += "- ‚úÖ Research data gathered\n- ‚è≥ Generating report title...\n"
        yield output, None, None
        
        report_title = generate_report_title(query, research_data)
        
        output += f"- ‚úÖ Report title generated: \"{report_title}\"\n"
        output += "- ‚è≥ Creating research outline...\n"
        yield output, None, None
        
        # Step 4: Create outline
        outline = create_research_outline(query, questions, answers, research_data)
        
        # Add outline to output
        output += "- ‚úÖ Research outline created\n\n## Research Outline:\n\n"
        output += outline + "\n\n"
        output += "- ‚è≥ Generating content section by section...\n"
        yield output, None, None
        
        # Parse the outline
        sections = parse_outline(outline)
        
        # Step 5: Generate content section by section
        content_sections = ""
        previous_content = ""
        
        for i, section in enumerate(sections):
            # Update progress
            output += f"- ‚è≥ Writing section {i+1}/{len(sections)}: {section['title']}...\n"
            yield output, None, None
            
            # Generate content for this section
            section_content = generate_section_content(
                query, 
                section, 
                previous_content, 
                research_data, 
                qa_context
            )
            
            # Add to previous content for context in next sections
            previous_content += f"\n\n## {section['title']}\n\n{section_content}"
            
            # Add to content sections
            content_sections += f"\n\n## {section['title']}\n\n{section_content}"
        
        # Generate executive summary and conclusion separately after all sections
        output += "- ‚úÖ All sections completed\n- ‚è≥ Generating executive summary and conclusion...\n"
        yield output, None, None
        
        executive_summary = generate_executive_summary(query, content_sections)
        conclusion = generate_conclusion(query, content_sections)
        
        # Assemble final report
        final_report = f"""# {report_title}

## Executive Summary

{executive_summary}

{content_sections}

## Conclusion

{conclusion}

---
*Research conducted using Groq Compound and Llama 4 Maverick*
"""
        
        # Generate file for download
        logger.info("Creating downloadable report file")
        report_files = save_report_to_file(final_report, report_title)
        
        # Final output
        output += "- ‚úÖ Report completed\n\n"
        output += "# Final Research Report\n\n"
        output += final_report
        
        # Make the download button visible with the report file
        yield output, final_report, report_files
        
    except Exception as e:
        logger.error(f"Error in research process: {str(e)}")
        error_message = f"# Error in Research Process\n\nAn error occurred during the research process: {str(e)}\n\nPlease try again with a different query or check your API key configuration."
        yield error_message, None, None

# Create Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# üî¨ Compound Research Assistant")
    gr.Markdown("Enter a research query to begin a comprehensive, automated research process. The system will generate a detailed report with hyperlinked sources, organized by sections.")
    
    # Main interface components
    query_input = gr.Textbox(
        label="Research Query", 
        placeholder="Enter your research question here...",
        lines=2
    )
    start_button = gr.Button("Start Research Process")
    
    # Output display with automatic updates
    output_display = gr.Markdown()
    
    # Hidden text state for storing the final report content
    report_content = gr.State()
    
    # Download button (initially visible=False is set in the yield statements)
    download_button = gr.File(label="Download Research Report")
    
    # Connect the button to the research process
    start_button.click(
        fn=compound_research_process,
        inputs=query_input,
        outputs=[output_display, report_content, download_button]
    )

if __name__ == "__main__":
    demo.launch(share=True)